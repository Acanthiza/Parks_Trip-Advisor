---
title: "Analysis of Trip Advisor data relating to South Australian parks"
author:
- Department for Environment and Water
date: "`r format(Sys.time(), '%A, %d %B, %Y')`"
output:
  bookdown::gitbook:
    split_by: chapter
    toc_depth: 3
    css: style.css
    keep_md: no
csl:                          "common/BibStyle.csl"
bibliography:                 ["common/refs.bib","rPackages.bib","common/RC.bib"]
link-citations: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, warning = FALSE, echo = FALSE, message = FALSE}

  useCores <- round(parallel::detectCores()*3/4,0)

  packages <- c("knitr"
                ,"bookdown"
                ,"tidyverse"
                ,"readxl"
                ,"fs"
                ,"lubridate"
                ,"rstan"
                ,"rstanarm"
                ,"ggridges"
                #,"tmap"
                ,"tidytext"
                )
  
  purrr::walk(packages,library,character.only=TRUE)
  
  write_bib(c("base",packages),file="rPackages.bib",tweak=TRUE)
  
  source("common/functions.R")
  
  # Set default chunk options (can adjust individual chunks differently if required)
  knitr::opts_chunk$set(warning = FALSE
                        , message = FALSE
                        , echo = FALSE
                        )
  
  options(knitr.kable.NA = "")
  
  # Set mapping defaults
  # tmap_options(basemaps = c("OpenStreetMap.Mapnik"
  #                           , "Esri.WorldImagery"
  #                           )
  #              )
  
  # reset ggplot default theme (rstanarm changes it for some reason)
  theme_set(theme_grey())
  
```
  
```{r data}

  dat <- dir_info("data") %>%
    dplyr::filter(grepl("xlsx$",path)) %>%
    dplyr::mutate(data = map(path, read_excel)) %>%
    dplyr::select(data) %>%
    tidyr::unnest() %>%
    dplyr::mutate(User = gsub(" R.$","",User)
                  , Year = year(Date)
                  , Month = format(Date,"%B")
                  , month = as.numeric(format(Date,"%m"))
                  , Month = fct_reorder(Month,month)
                  , Yearmon = as_date(paste0("01/",Month,"/",Year),tz=Sys.timezone(),format="%d/%B/%Y")
                  , Age = gsub("18-34","18-24",Age)
                  , id = row_number()
                  ) %>%
    dplyr::mutate_if(is.character,list(~gsub("^Null$|^Not given$",NA,.))) %>%
    dplyr::select(-month) %>%
    dplyr::filter(!grepl("13",Age))

```

# Introduction

DEW manages many parks across South Australia. Some of these parks have a 'page' on [TripAdvisor](https://www.tripadvisor.com.au/). For example, there is a page for [Ikara-Flinders Ranges National Park](https://www.tripadvisor.com.au/Attraction_Review-g499711-d6456334-Reviews-Flinders_Ranges_National_Park-Hawker_Flinders_Ranges_South_Australia.html).

DEW are interested to know what information is contained in TripAdvisor reviews. Specifically, the following questions were identified by the community engagement branch:

* Where are visitors from:
     + Overall percentages – as in, of all the people who reviewed SA parks, 40 percent were Australian, 10 percent were English, 8 percent were American etc.
     + By park percentages – as in, of all the people who reviewed Belair National Park, 40 percent were Australian, 10 percent were English, 8 percent were American etc.
* What gender are visitors:
    + Overall percentages – as in, of all the people who reviewed SA parks, 45 percent were male and 55 percent were female
    + By park percentages – as in, of all the people who reviewed Belair national park, 45 percent were male and 55 percent were female
* What ages are visitors?
    + Overall
    + By individual park
* When are reviews made? (As in, in which month is the quantity of reviews submitted to Trip Advisor the highest? In which month is the quantity of reviews submitted to Trip Advisor the lowest?)
    + Overall
    + By individual park
* How does seasonality affect park ratings? (As in, parks receive most positive ratings in December and most negative ratings in May)
    + Overall
    + By individual park
* Are males or females more likely to rate parks positively or negatively?
    + Overall
    + By individual park
* Which country/state is more likely to rate parks positively or negatively?
    + Overall
    + By individual park
* Which age is more likely to rate parks positively or negatively?
    + Overall
    + By individual park
* Other possible questions
    + SA analysis – Adelaide vs regional views of parks
    + Multiple park reviews from one user:
    + Does reviewing more than one park lead to higher or lower ratings?
    + What parks are visited by people who review more than one park? Can we get a sense of itinerary from international visitors?
    
After an initial data exploration, these questions were treated as the following:

* What is the effect of age and gender on the rating given?
* What is the effect of the park visited on the rating given?
* What is the effect of the origin of a reviewer on the rating given?

These three analyses answer most of the questions posed by the community engagement branch but do not always examine interations, such as, say, the interaction of age and park on stars. This was done as sample size was very small for some groups, especially for low stars.

A further analysis of words used in the titles and reviews was also undertaken.

# Methods

## Data source

Data were taken from Trip Advisor. The site is one of the most popular sources of reviews for hotels, restaurants, experiences, attractions and places such as parks. Trip Advisor features user-generated content with 315 million reviewers (active and inactive) and about 500 million reviews [Wikipedia](https://en.wikipedia.org/wiki/TripAdvisor). 

For the purposes of this analysis, the search function in Trip Advisor was used to identify all parks, reserves and features in parks and reserves that have been reviewed. Some reviews were captured under a park name, such as "Flinders Chase National Park" and some were captured under a feature name, such as "Flinders Chase Visitor Centre". 

In order to manage the scope of this analysis it was decided to collect reviews in the first instance for parks that were reviewed under their official park names. Where there was a park that is of importance to DEW that had reviews under a feature name rather than a park name, these reviews were also collected. For example, reviews were collected for "Waterfall Gully" rather than for "Cleland Conservation Park".

In order to manage the scope of the analysis, and in reflection of DEW’s current organisational structure, it was also decided to limit the review collection to parks and reserves that are not "iconic sites". DEW's organisational structure separates staff working on "iconic sites" that usually charge a per-person fee, such as Cleland Wildlife Park, Naracoorte Caves and Seal Bay; and staff working on sites that usually have no fee, or a vehicle entry fee, such as Flinders Chase National Park, Flinders Ranges National Park or Belair National Park. 

The reviews available were cross referenced against a list of priority parks that DEW has identified through the parks characterisation project to ensure that reviews were captured for all significant non-iconic sites. 

The web scraping software [Octoparse](https://www.octoparse.com/) was used to capture the following elements for analysis: 

* Reviewer name 
* Reviewer location (country, state and city) 
* Reviewer age 
* Reviewer gender 
* Review date 
* Review title 
* Review text 
* Review star rating 

Note that the web scraping software was able to effectively capture elements such as name, date, title, text and star rating, but that capturing the demographic information about the reviewers was a more difficult process. The demographic information is contained within "hover over" elements of the Trip Advisor web pages. Code was written to scrape these elements for the initial data scrape; manual cutting and pasting was used for scrape updates. It may be for this reason that most academic analysis of Trip Advisor reviews does not extend to considering demographic information and that this report may offer some new information about the utility of this data source. 

Once the data was captured it was cleaned so that it could be imported into SPSS for quantitative analysis and Nvivo for qualitative analysis. The cleaning included:

* Ensuring that each review was attached to a unique reviewer name to allow for import into Nvivo. For example, where a reviewer had written reviews about more than one park, R1, R2, R3 and so on were added to their name: “AussieGirl R1”, “AussieGirl R2” etc. 
* Standardising place names to ensure that spellings were correct. 
* Allocating state and/or countries to reviewers who gave only partial location information such as city of residence. 
* Ensuring that the data set contained no blank cells.
* Removing reviewers under 18 years old (Age field equal to '13-18').

# Data exploration

## Summary

Between `r format(min(dat$Date),"%d/%B/%Y")` and `r format(max(dat$Date),"%d/%B/%Y")` there were `r nrow(dat)` reviews on TripAdvisor meeting the criteria outline in the [methods].

## Missing data

```{r dataMissing}

  datMiss <- dat %>%
    summarise_all(list(~sum(is.na(.)))) %>%
    t() %>%
    data.frame() %>%
    rownames_to_column() %>%
    dplyr::rename(column = 1, NAs = 2) %>%
    dplyr::mutate(per = 100*NAs/nrow(dat)
                  , column = fct_inorder(column)
                  )

```

Some of the key data fields were left blank in many reviews. For example, `r sum(is.na(dat$Gender))` (`r round(100*sum(is.na(dat$Gender))/nrow(dat),0)`%) reviews did not provide their gender and `r sum(is.na(dat$Age))` (`r round(100*sum(is.na(dat$Age))/nrow(dat),0)`%) didn't provide their age. Figure \@ref(fig:plotMissing) shows the percentage of reviews that did not provide information against each field.

```{r plotMissing, fig.cap = "A large proportion of reviews did not answer some questions, particularly Gender and Age"}

  ggplot(datMiss, aes(column,per)) +
    geom_col() +
    theme(axis.text.x = element_text(angle = 90)) +
    labs(x = "Column"
         , y = "Percentage of null (NA) cells in each column"
         )

```

## Count of reviews

```{r dataCount}
  
  datCount <- dat %>%
    dplyr::select(-Title,-Review,-Yearmon,-Date) %>%
    tidyr::gather(variable,value,1:ncol(.)) %>%
    dplyr::count(variable,value) 

```

There are several variables in the data with relatively few levels: `r vec_to_sentence(unique(pull(datCount,variable)),";")`. Figure \@ref(fig:plotDiscrete) shows the most frequently occuring values in each of those variables.

```{r plotDiscrete, fig.height = 10, fig.cap = "Most frequent values in each of the variables. Most reviews are from Adelaide based 50-64 year olds called Charmaine who visit Flinders Chase in January"}
  
  ggplot(datCount %>%
           dplyr::filter(n > 4)%>%
           dplyr::group_by(variable) %>%
           dplyr::top_n(12) %>%
           dplyr::ungroup() %>%
           dplyr::mutate(value = fct_reorder(value,n))
         ) +
    geom_col(aes(value,n)) +
    facet_wrap(~variable, scales = "free",ncol=2) +
    coord_flip() +
    theme(axis.text.x = element_text(angle = 90)) +
    labs(x = "Value of variable"
         , y = "Count of value"
         )

```


# Age and gender

```{r model}

  modFile <- "out/model_stars-by-age-gender-year.rds"

  # variables to model
  varMod <- c("Y"
              , "time"
              , "Month"
              , "Age"
              , "Gender"
              , "Park"
              , "State"
              )
  
  datMod <- dat %>%
    dplyr::filter(Year > 2011, Year < 2019) %>%
    dplyr::mutate(Y = Stars -1
                  , medYear = median(Year)
                  , time = Year - medYear
                  , State = fct_lump(State, n = 10)
                  , Park = fct_lump(Park, n = 10)
                  ) %>%
    dplyr::select(varMod) %>%
    na.omit()
    
  if(file_exists(modFile)) {
    
    mod <- read_rds(modFile)
    
  } else {
    
    mod <-
      
      stan_glmer(
        as.formula(paste("cbind(Y,4-Y) ~ Gender*Age*time + (1|Month) + (1|Park) + (1|State)")) #binomial
        , data = datMod
        , family = binomial
        #, chains = 5
        #, iter = 500
        , cores = useCores
      )
    
    saveRDS(mod,modFile)
    
  }

```

## Model

Trend through time (year) in stars given by age and gender of reviewers (and the interaction of age, gender and time) was analysed using a Bayesian binomial generalised linear mixed model. The analysis was run using the rstanarm package [@RN4481] in R [@R-base]. Month (of review), Park (visited by reviewer) and State (of origin of reviewer) were treated as random effects in the analysis. Each reviewer was then assumed to provide an independent data point for the analysis. A time field was generated as $time = min(Year) + (max(Year) - min(Year))/2$. The model specification was:

``r format(formula(mod))`` where `Y` is Stars-1.

## Model diagnostics

Figure \@ref(fig:mixing) shows that each of the chains used to estimate model parameters converged around similar values for each parameter.

```{r mixing, fig.cap = "All chains converged around the same values for each of the model paramaters (first 10 shown here)"}

  stan_trace(mod)

```

---

Figure \@ref(fig:rhat) provides further support for convergence of the chains as no values are markedly different to 1.0.

```{r rhat, fig.cap = "Rhat values show no reason for concern"}

  plot(mod, "rhat_hist")

```

---

Figure \@ref(fig:fit) shows how (a sample of) the model runs (in light blue) compare with the original data (shown in dark blue).

```{r fit, fig.cap = "The model runs under estimate four stars and over estimate three stars relative to the data"}

  # Model fit
  pp_check(mod)

```

---

Figure \@ref(fig:meanVsSd) shows how the mean and standard deviation of the model runs (in light blue) compare with that of the original data (in dark blue).

```{r meanVsSd, fig.cap = "Collectively the model runs do a reasonable job estimating the data"}

  pp_check(mod, plotfun = "stat_2d", stat = c("mean", "sd"))

```

---

## Model predictions

In order to better understand how the data and resulting model are able to inform management, a number (4000) of simulations were made based on the model results (strictly, draws were made from the posterior predictive distribution). Each simulation generated a prediction based on a single draw of the model parameters from their predicted distributions. Table \@ref(tab:predict) shows a summary of these results.

```{r predict}

  # Use the model to predict results over variables of interest
  modPred <- datMod %>%
    dplyr::select(time,Age,Gender) %>%
    unique() %>%
    dplyr::mutate(col = row.names(.)
                  , Y = 4
                  , medYear = median(dat$Year)
                  ) %>%
    dplyr::left_join(as_tibble(posterior_predict(mod
                                                 , newdata = .
                                                 , re.form = NA
                                                 )
                               ) %>%
                       tibble::rownames_to_column(var = "row") %>%
                       tidyr::gather(col,value,2:ncol(.))
                     ) %>%
    (function(x) dplyr::bind_cols(x %>% dplyr::select(-value),value = as.numeric(x$value))) %>%
    dplyr::mutate(Year = unique(.$medYear) + time
                  , Stars = value + 1
                  #, Month = as.factor(Month, levels = levels(datMod$Month))
                  )

  # summarise the results
  modRes <- as_tibble(modPred) %>%
    dplyr::group_by(Year,Age,Gender) %>%
    dplyr::summarise(n = n()
                     , nCheck = nrow(as_tibble(mod))
                     , modMedian = quantile(Stars,0.5)
                     , Mean = mean(Stars)
                     , modci90lo = quantile(Stars, 0.025)
                     , modci90up = quantile(Stars, 0.975)
                     , ci = modci90up-modci90lo
                     , text = paste0(round(modMedian,2)," (",round(modci90lo,2)," to ",round(modci90up,2),")")
                     ) %>%
    dplyr::ungroup() %>%
    dplyr::mutate_if(is.numeric,round,2)
  
  kable(modRes %>%
          dplyr::select(Year,Age,Gender,Mean,`Median and range in 90% credible intervals`=text) %>%
          dplyr::group_by(Age,Gender) %>%
          dplyr::filter(Year == max(Year)) %>%
          dplyr::ungroup()
        , caption = "Summary of model results"
        )

```

---

Figure \@ref(fig:modResult) shows a plot of the model results. Overwhelmingly, reviewers give 5 stars, irrespective of age or gender.

```{r modResult, fig.cap = "Model results - mean credible intervals. Most reviewers give 5 stars irrespective of age or gender. The lower 95% credible intervals go to three stars in almost every case - i.e. more than 95% of responses are three stars or more. Only for 18-24 year old males in 2015 do the 95% credible intervals go to 1 star"}

  ggplot(data=modRes,aes(Year,Mean,group=Age)) +
    geom_line() +
    geom_ribbon(aes(ymin = modci90lo, ymax = modci90up, fill = Age), alpha = 0.6) +
    scale_colour_viridis_d() +
    scale_fill_viridis_d() +
    facet_grid(Age~Gender
               #, scales="free"
               ) +
    labs(y = "Stars"
         , subtitle = "Shading represents 95% credible intervals"
         )

```

# Park

```{r model2}

  modFile <- "out/model_stars-by-park.rds"

  # variables to model
  varMod <- c("Y"
              , "time"
              #, "Month"
              , "Age"
              , "Gender"
              , "Park"
              , "State"
              )
  
  datMod <- dat %>%
    dplyr::filter(Year > 2011, Year < 2019) %>%
    dplyr::mutate(Y = Stars -1
                  , medYear = median(Year)
                  , time = Year - medYear
                  , State = fct_lump(State, n = 10)
                  , Park = fct_lump(Park, n = 10)
                  ) %>%
    dplyr::select(varMod) %>%
    na.omit()
    
  if(file_exists(modFile)) {
    
    mod <- read_rds(modFile)
    
  } else {
    
    mod <-
      
      stan_glmer(
        as.formula(paste("cbind(Y,4-Y) ~ time*Park + (1|Age) + (1|Gender) + (1|State)")) #binomial
        , data = datMod
        , family = binomial
        , adapt_delta = 0.99
        #, chains = 5
        #, iter = 500
        , cores = useCores
      )
    
    saveRDS(mod,modFile)
    
  }

```

## Model

Trend through time (year) in stars given to a park (and the interaction of park and time) was analysed using a Bayesian binomial generalised linear mixed model. The analysis was run using the rstanarm package [@RN4481] in R [@R-base]. Age, gender and origin of reviewer were treated as random effects in the analysis. Each reviewer was then assumed to provide an independent data point for the analysis. A time field was generated as $time = min(Year) + (max(Year) - min(Year))/2$. The model specification was:

``r format(formula(mod))`` where `Y` is Stars-1.

## Model diagnostics

Figure \@ref(fig:mixing2) shows that each of the chains used to estimate model parameters converged around similar values for each parameter.

```{r mixing2, fig.cap = "All chains converged around the same values for each of the model paramaters (first 10 shown here)"}

  stan_trace(mod)

```

---

Figure \@ref(fig:rhat2) provides further support for convergence of the chains as no values are markedly different to 1.0.

```{r rhat2, fig.cap = "Rhat values show no reason for concern"}

  plot(mod, "rhat_hist")

```

---

Figure \@ref(fig:fit2) shows how (a sample of) the model runs (in light blue) compare with the original data (shown in dark blue).

```{r fit2, fig.cap = "The model runs under estimate four stars and over estimate three stars relative to the data"}

  # Model fit
  pp_check(mod)

```

---

Figure \@ref(fig:meanVsSd2) shows how the mean and standard deviation of the model runs (in light blue) compare with that of the original data (in dark blue).

```{r meanVsSd2, fig.cap = "Collectively the model runs do a reasonable job estimating the data"}

  pp_check(mod, plotfun = "stat_2d", stat = c("mean", "sd"))

```

---

## Model predictions

In order to better understand how the data and resulting model are able to inform management, a number (4000) of simulations were made based on the model results (strictly, draws were made from the posterior predictive distribution). Each simulation generated a prediction based on a single draw of the model parameters from their predicted distributions. Table \@ref(tab:predict2) shows a summary of these results.

```{r predict2}

  # Use the model to predict results over variables of interest
  modPred <- datMod %>%
    dplyr::select(time,Park) %>%
    unique() %>%
    dplyr::mutate(col = row.names(.)
                  , Y = 4
                  , medYear = median(dat$Year)
                  ) %>%
    dplyr::left_join(as_tibble(posterior_predict(mod
                                                 , newdata = .
                                                 , re.form = NA
                                                 )
                               ) %>%
                       tibble::rownames_to_column(var = "row") %>%
                       tidyr::gather(col,value,2:ncol(.))
                     ) %>%
    (function(x) dplyr::bind_cols(x %>% dplyr::select(-value),value = as.numeric(x$value))) %>%
    dplyr::mutate(Year = unique(.$medYear) + time
                  , Stars = value + 1
                  #, Month = as.factor(Month, levels = levels(datMod$Month))
                  )

  # summarise the results
  modRes2 <- as_tibble(modPred) %>%
    dplyr::group_by(Year,Park) %>%
    dplyr::summarise(n = n()
                     , nCheck = nrow(as_tibble(mod))
                     , modMedian = quantile(Stars,0.5)
                     , Mean = mean(Stars)
                     , modci90lo = quantile(Stars, 0.025)
                     , modci90up = quantile(Stars, 0.975)
                     , ci = modci90up-modci90lo
                     , text = paste0(round(modMedian,2)," (",round(modci90lo,2)," to ",round(modci90up,2),")")
                     ) %>%
    dplyr::ungroup() %>%
    dplyr::left_join(datMod %>% dplyr::count(Park) %>% dplyr::rename(reviews=n)) %>%
    dplyr::mutate_if(is.numeric,round,2) %>%
    dplyr::mutate(label = Park
                  , label = paste0(label,": ",reviews," reviews")
                  )
  
  modRes2Tab <- modRes2 %>%
          dplyr::select(Park,Year,Mean,`Median and range in 90% credible intervals`=text) %>%
          dplyr::group_by(Park) %>%
          dplyr::filter(Year == max(Year)) %>%
          dplyr::ungroup() %>%
          dplyr::mutate(Park = fct_reorder(Park,Mean,.desc = TRUE)) %>%
          dplyr::arrange(Park)
  
  kable(modRes2Tab
        , caption = paste0("Summary of model results for the last year. ", modRes2Tab %>% dplyr::slice((nrow(.)-2):nrow(.)) %>% pull(Park) %>% vec_to_sentence(), " have the lowest stars (based on mean credible estimates)")
        )
  

```

---

```{r modResult2, fig.cap = "Model results - mean credible intervals. Belair and Point Labatt both seem to be getting increasingly negative reviews - although the mean credible estimate for both remains above 4 stars currently"}

  ggplot(data=modRes2,aes(Year,Mean,fill=Park)) +
    geom_line() +
    geom_ribbon(aes(ymin = modci90lo, ymax = modci90up), alpha = 0.5) +
    facet_wrap(~label
               , labeller = labeller(label = label_wrap_gen(20))
               ) +
    labs(y = "Mean credible value for Stars"
         , subtitle = "Shading represents 95% credible intervals"
         ) +
    theme(axis.text.x = element_text(angle = 90)
          , legend.position = "none"
          ) +
    scale_fill_viridis_d(end = 0.9)

```

# Origin

```{r model3}

  modFile <- "out/model_stars-by-origin.rds"

  # variables to model
  varMod <- c("Y"
              , "time"
              #, "Month"
              , "Age"
              , "Gender"
              , "Park"
              , "State"
              )
  
  datMod <- dat %>%
    dplyr::filter(Year > 2011, Year < 2019) %>%
    dplyr::mutate(Y = Stars -1
                  , medYear = median(Year)
                  , time = Year - medYear
                  , State = fct_lump(State, n = 10)
                  , Park = fct_lump(Park, n = 10)
                  ) %>%
    dplyr::select(varMod) %>%
    na.omit()
    
  if(file_exists(modFile)) {
    
    mod <- read_rds(modFile)
    
  } else {
    
    mod <-
      
      stan_glmer(
        as.formula(paste("cbind(Y,4-Y) ~ time*State + (1|Age) + (1|Gender) + (1|Park)")) #binomial
        , data = datMod
        , family = binomial
        , adapt_delta = 0.99
        #, chains = 5
        #, iter = 500
        , cores = useCores
      )
    
    saveRDS(mod,modFile)
    
  }

```

## Model

Trend through time (year) in stars given by the origin of reviewers (and the interaction of origin and time) was analysed using a Bayesian binomial generalised linear mixed model. The analysis was run using the rstanarm package [@RN4481] in R [@R-base]. Age and gender of the reviewer and the park visisted were treated as random effects in the analysis. Each reviewer was then assumed to provide an independent data point for the analysis. A time field was generated as $time = min(Year) + (max(Year) - min(Year))/2$. The model specification was:

``r format(formula(mod))`` where `Y` is Stars-1.

## Model diagnostics

Figure \@ref(fig:mixing3) shows that each of the chains used to estimate model parameters converged around similar values for each parameter.

```{r mixing3, fig.cap = "All chains converged around the same values for each of the model paramaters (first 10 shown here)"}

  stan_trace(mod)

```

---

Figure \@ref(fig:rhat3) provides further support for convergence of the chains as no values are markedly different to 1.0.

```{r rhat3, fig.cap = "Rhat values show no reason for concern"}

  plot(mod, "rhat_hist")

```

---

Figure \@ref(fig:fit3) shows how (a sample of) the model runs (in light blue) compare with the original data (shown in dark blue).

```{r fit3, fig.cap = "The model runs under estimate four stars and over estimate three stars relative to the data"}

  # Model fit
  pp_check(mod)

```

---

Figure \@ref(fig:meanVsSd3) shows how the mean and standard deviation of the model runs (in light blue) compare with that of the original data (in dark blue).

```{r meanVsSd3, fig.cap = "Collectively the model runs do a reasonable job estimating the data"}

  pp_check(mod, plotfun = "stat_2d", stat = c("mean", "sd"))

```

---

## Model predictions

In order to better understand how the data and resulting model are able to inform management, a number (4000) of simulations were made based on the model results (strictly, draws were made from the posterior predictive distribution). Each simulation generated a prediction based on a single draw of the model parameters from their predicted distributions. Table \@ref(tab:predict3) shows a summary of these results.

```{r predict3}

  # Use the model to predict results over variables of interest
  modPred <- datMod %>%
    dplyr::select(time,State) %>%
    unique() %>%
    dplyr::mutate(col = row.names(.)
                  , Y = 4
                  , medYear = median(dat$Year)
                  ) %>%
    dplyr::left_join(as_tibble(posterior_predict(mod
                                                 , newdata = .
                                                 , re.form = NA
                                                 )
                               ) %>%
                       tibble::rownames_to_column(var = "row") %>%
                       tidyr::gather(col,value,2:ncol(.))
                     ) %>%
    (function(x) dplyr::bind_cols(x %>% dplyr::select(-value),value = as.numeric(x$value))) %>%
    dplyr::mutate(Year = unique(.$medYear) + time
                  , Stars = value + 1
                  #, Month = as.factor(Month, levels = levels(datMod$Month))
                  )

  # summarise the results
  modRes3 <- as_tibble(modPred) %>%
    dplyr::group_by(Year,State) %>%
    dplyr::summarise(n = n()
                     , nCheck = nrow(as_tibble(mod))
                     , modMedian = quantile(Stars,0.5)
                     , Mean = mean(Stars)
                     , modci90lo = quantile(Stars, 0.025)
                     , modci90up = quantile(Stars, 0.975)
                     , ci = modci90up-modci90lo
                     , text = paste0(round(modMedian,2)," (",round(modci90lo,2)," to ",round(modci90up,2),")")
                     ) %>%
    dplyr::ungroup() %>%
    dplyr::left_join(datMod %>% dplyr::count(State) %>% dplyr::rename(reviews=n)) %>%
    dplyr::mutate_if(is.numeric,round,2) %>%
    dplyr::mutate(label = State
                  , label = paste0(label,": ",reviews," reviews")
                  )
  
  modRes3Tab <- modRes3 %>%
          dplyr::select(State,Year,Mean,`Median and range in 90% credible intervals`=text) %>%
          dplyr::group_by(State) %>%
          dplyr::filter(Year == max(Year)) %>%
          dplyr::ungroup() %>%
          dplyr::mutate(State = fct_reorder(State,Mean,.desc=TRUE)) %>%
          dplyr::arrange(State)
  
  kable(modRes3Tab
        , caption = paste0("Summary of model results for the last year. ", modRes3Tab %>% dplyr::slice(1:2) %>% pull(State) %>% vec_to_sentence(), " give the highest stars (based on mean credible estimates)")
        )

```

---

```{r modResult3, fig.cap = paste0("Model results - mean credible intervals. California and the North Island of New Zealand both appear to have increased the stars they give in reviews through time, however the total number of reviews are low in each case so this result should be treated cautiously. There is very little trend in stars given by origin of reviewers for other origins")}

  ggplot(data=modRes3,aes(Year,Mean,fill=State)) +
    geom_line() +
    geom_ribbon(aes(ymin = modci90lo, ymax = modci90up), alpha = 0.5) +
    scale_fill_viridis_d(end=0.9) +
    facet_wrap(~label
               , labeller = labeller(label = label_wrap_gen(20))
               ) +
    labs(y = "Mean credible value for Stars"
         , subtitle = "Shading represents 95% credible intervals"
         ) +
    theme(axis.text.x = element_text(angle = 90)
          , legend.position = "none"
          )

```

# Word analysis

## Overall sentiment

```{r dataTitle}

  fixSentiment <- c("wild","desert","falls","unbelievable","unusual","unexpected","miss","challenging","remarkable","steep","cold","dirt","rocky","crashing","downhill","shady","ruins","cave","wedge")

  datTitle <- dat %>%
    tidytext::unnest_tokens(word,Title,drop = FALSE) %>%
    dplyr::anti_join(tidytext::stop_words) %>%
    dplyr::left_join(tidytext::get_sentiments("bing")) %>%
    dplyr::mutate(sentiment = ifelse(word %in% fixSentiment,NA,sentiment))
  
  datReview <- dat %>%
    tidytext::unnest_tokens(word,Review,drop = FALSE) %>%
    dplyr::anti_join(tidytext::stop_words) %>%
    dplyr::left_join(tidytext::get_sentiments("bing")) %>%
    dplyr::mutate(sentiment = ifelse(word %in% fixSentiment,NA,sentiment))
  
```

Using the tidytext package [@R-tidytext], the text within the character fields `Title` and `Review` were analysed. Figure \@ref(fig:title) shows the ten most common positive, negative and neutral words given in titles. Figure \@ref(fig:review) shows the same for reviews. The following words were reclassified as neutral from negative sentiment as they were overwhelmingly not used in a negative manner: `r vec_to_sentence(fixSentiment,";")`.

```{r title, fig.cap = "Titles had words with positive sentiment much more frequently than words with negative sentiment"}

  datForPlot <- datTitle %>%
    dplyr::count(word, sentiment, sort = TRUE) %>%
    dplyr::group_by(sentiment) %>%
    dplyr::slice(1:10) %>%
    dplyr::ungroup() %>%
    dplyr::mutate(word = fct_reorder(word,n)
                  , sentiment = fct_explicit_na(sentiment, na_level = "neutral")
                  , sentiment = fct_relevel(sentiment, "positive", "negative")
                  )
            
  ggplot(datForPlot, aes(word,n,fill=sentiment)) +
    geom_col() +
    coord_flip() +
    facet_grid(sentiment~., scales = "free_y", space = "free_y") +
    scale_fill_viridis_d(end = 0.8)

```

```{r review, fig.cap = "Reviews used words with positive sentiment more frequently than words with negative sentiment"}

  datForPlot <- datReview %>%
    dplyr::count(word, sentiment, sort = TRUE) %>%
    dplyr::group_by(sentiment) %>%
    dplyr::slice(1:10) %>%
    dplyr::ungroup() %>%
    dplyr::mutate(word = fct_reorder(word,n)
                  , sentiment = fct_explicit_na(sentiment, na_level = "neutral")
                  , sentiment = fct_relevel(sentiment, "positive", "negative")
                  )
            
  ggplot(datForPlot, aes(word,n,fill=sentiment)) +
    geom_col() +
    coord_flip() +
    facet_grid(sentiment~., scales = "free_y", space = "free_y") +
    scale_fill_viridis_d(end = 0.8)

```

## By park sentiment

```{r parkTitle, fig.cap = "Top ten words in the title text for each sentiment class (positive, negative and neutral) for the parks with the most reviews"}

  datForPlot <- datTitle %>%
    dplyr::group_by(Park) %>%
    dplyr::mutate(reviews = n_distinct(id)) %>%
    dplyr::ungroup() %>%
    dplyr::count(id, word, Park, reviews, sentiment, sort = TRUE) %>%
    dplyr::count(word, Park, reviews, sentiment, sort = TRUE) %>%
    dplyr::filter(reviews > quantile(unique(reviews), probs = 0.8)) %>%
    dplyr::group_by(Park,sentiment) %>%
    dplyr::slice(1:10) %>%
    dplyr::ungroup() %>%
    dplyr::arrange(Park,n) %>%
    dplyr::mutate(order = row_number()
                  , per = 100*n/reviews
                  , sentiment = fct_explicit_na(sentiment, na_level = "neutral")
                  , sentiment = fct_relevel(sentiment, "positive", "negative")
                  )
            
  ggplot(datForPlot, aes(order,per,fill=sentiment)) +
    geom_bar(stat = "identity") +
    facet_wrap(~Park, scales = "free_y", labeller = labeller(Park = label_wrap_gen(20))) +
    scale_x_continuous(breaks = datForPlot$order
                       , labels = datForPlot$word
                       , expand = c(0,0)
                       ) +
    scale_fill_viridis_d(end = 0.8) +
    coord_flip() +
    labs(y = "Percentage of reviews"
         , x = "Word"
         )

```

```{r parkReview, fig.cap = "Top ten words in the review text for each sentiment class (positive, negative and neutral) for the parks with the most reviews"}

  datForPlot <- datReview %>%
    dplyr::group_by(Park) %>%
    dplyr::mutate(reviews = n_distinct(id)) %>%
    dplyr::ungroup() %>%
    dplyr::count(id, word, Park, reviews, sentiment, sort = TRUE) %>%
    dplyr::count(word, Park, reviews, sentiment, sort = TRUE) %>%
    dplyr::filter(reviews > quantile(unique(reviews), probs = 0.8)) %>%
    dplyr::group_by(Park,sentiment) %>%
    dplyr::slice(1:10) %>%
    dplyr::ungroup() %>%
    dplyr::arrange(Park,n) %>%
    dplyr::mutate(order = row_number()
                  , per = 100*n/reviews
                  , sentiment = fct_explicit_na(sentiment, na_level = "neutral")
                  , sentiment = fct_relevel(sentiment, "positive", "negative")
                  )
            
  ggplot(datForPlot, aes(order,per,fill=sentiment)) +
    geom_bar(stat = "identity") +
    facet_wrap(~Park, scales = "free_y", labeller = labeller(Park = label_wrap_gen(20))) +
    scale_x_continuous(breaks = datForPlot$order
                       , labels = datForPlot$word
                       , expand = c(0,0)
                       ) +
    scale_fill_viridis_d(end = 0.8) +
    coord_flip() +
    labs(y = "Percentage of reviews"
         , x = "Word"
         )
  
```

# Discussion

## Questions

It is now possible to answer the questions posed at the start with respect to the TripAdvisor data.

### What is the effect of age and gender on the rating given?

There is very little effect of age and gender on stars. 18-24 year old males had the lowest mean credible estimate for stars but this was still a relatively healthy `r modRes %>% dplyr::filter(Gender == "Male", Age =="18-24") %>% dplyr::filter(Year == max(Year)) %>% dplyr::pull(Mean)` stars.

### What is the effect of the park visited on the rating given?

There is little effect of park on the rating given. In the last year there was only `r modRes2 %>% dplyr::filter(Year == max(Year)) %>% dplyr::mutate(diff=Mean-min(Mean)) %>% pull(diff) %>% max()` stars difference between the maximum and minimum mean credible estimate for stars given to parks in the dataset. This is only `r 100*(modRes2 %>% dplyr::filter(Year == max(Year)) %>% dplyr::mutate(diff=Mean-min(Mean)) %>% pull(diff) %>% max())/5`% of the possible range of 1 to 5.

### What is the effect of the origin of a reviewer on the rating given?

There is very little effect of the origin of a reviewer on the rating given. In the last year, there was only `r modRes3 %>% dplyr::filter(Year == max(Year)) %>% dplyr::mutate(diff=Mean-min(Mean)) %>% pull(diff) %>% max()` stars or `r 100*(modRes3 %>% dplyr::filter(Year == max(Year)) %>% dplyr::mutate(diff=Mean-min(Mean)) %>% pull(diff) %>% max())/5`% of the possible difference.

### Summary

The TripAdvisor data has little resolution in stars given for the attributes available for analysis. Most reviewers give South Australian DEW managed parks very good reviews - roughly 4.5 stars on average.

## Words used in reviews

As for the stars analysis, the word analysis showed overwhelmingly positive words were used to describe both overall parks (Figures \@ref(fig:title) & \@ref(fig:review)) or for specific parks (Figures \@ref(fig:parkTitle) & \@ref(fig:parkReview)).

# References
