[
["introduction.html", "Analysis of Trip Advisor data relating to South Australian parks 1 Introduction", " Analysis of Trip Advisor data relating to South Australian parks Department for Environment and Water Thursday, 23 May, 2019 1 Introduction DEW manages many parks across South Australia. Some of these parks have a ‘page’ on TripAdvisor. For example, there is a page for Ikara-Flinders Ranges National Park. DEW are interested to know what information is contained in TripAdvisor reviews. Specifically, the following questions were identified by the community engagement branch: Where are visitors from: Overall percentages – as in, of all the people who reviewed SA parks, 40 percent were Australian, 10 percent were English, 8 percent were American etc. By park percentages – as in, of all the people who reviewed Belair National Park, 40 percent were Australian, 10 percent were English, 8 percent were American etc. What gender are visitors: Overall percentages – as in, of all the people who reviewed SA parks, 45 percent were male and 55 percent were female By park percentages – as in, of all the people who reviewed Belair national park, 45 percent were male and 55 percent were female What ages are visitors? Overall By individual park When are reviews made? (As in, in which month is the quantity of reviews submitted to Trip Advisor the highest? In which month is the quantity of reviews submitted to Trip Advisor the lowest?) Overall By individual park How does seasonality affect park ratings? (As in, parks receive most positive ratings in December and most negative ratings in May) Overall By individual park Are males or females more likely to rate parks positively or negatively? Overall By individual park Which country/state is more likely to rate parks positively or negatively? Overall By individual park Which age is more likely to rate parks positively or negatively? Overall By individual park Other possible questions SA analysis – Adelaide vs regional views of parks Multiple park reviews from one user: Does reviewing more than one park lead to higher or lower ratings? What parks are visited by people who review more than one park? Can we get a sense of itinerary from international visitors? After an initial data exploration, these questions were treated as the following: What is the effect of age and gender on the rating given? What is the effect of the park visited on the rating given? What is the effect of the origin of a reviewer on the rating given? These three analyses answer most of the questions posed by the community engagement branch but do not always examine interations, such as, say, the interaction of age and park on stars. This was done as sample size was very small for some groups, especially for low stars. A further analysis of words used in the titles and reviews was also undertaken. "],
["methods.html", "2 Methods 2.1 Data source", " 2 Methods 2.1 Data source Data were taken from Trip Advisor. The site is one of the most popular sources of reviews for hotels, restaurants, experiences, attractions and places such as parks. Trip Advisor features user-generated content with 315 million reviewers (active and inactive) and about 500 million reviews Wikipedia. For the purposes of this analysis, the search function in Trip Advisor was used to identify all parks, reserves and features in parks and reserves that have been reviewed. Some reviews were captured under a park name, such as “Flinders Chase National Park” and some were captured under a feature name, such as “Flinders Chase Visitor Centre”. In order to manage the scope of this analysis it was decided to collect reviews in the first instance for parks that were reviewed under their official park names. Where there was a park that is of importance to DEW that had reviews under a feature name rather than a park name, these reviews were also collected. For example, reviews were collected for “Waterfall Gully” rather than for “Cleland Conservation Park”. In order to manage the scope of the analysis, and in reflection of DEW’s current organisational structure, it was also decided to limit the review collection to parks and reserves that are not “iconic sites”. DEW’s organisational structure separates staff working on “iconic sites” that usually charge a per-person fee, such as Cleland Wildlife Park, Naracoorte Caves and Seal Bay; and staff working on sites that usually have no fee, or a vehicle entry fee, such as Flinders Chase National Park, Flinders Ranges National Park or Belair National Park. The reviews available were cross referenced against a list of priority parks that DEW has identified through the parks characterisation project to ensure that reviews were captured for all significant non-iconic sites. The web scraping software Octoparse was used to capture the following elements for analysis: Reviewer name Reviewer location (country, state and city) Reviewer age Reviewer gender Review date Review title Review text Review star rating Note that the web scraping software was able to effectively capture elements such as name, date, title, text and star rating, but that capturing the demographic information about the reviewers was a more difficult process. The demographic information is contained within “hover over” elements of the Trip Advisor web pages. Code was written to scrape these elements for the initial data scrape; manual cutting and pasting was used for scrape updates. It may be for this reason that most academic analysis of Trip Advisor reviews does not extend to considering demographic information and that this report may offer some new information about the utility of this data source. Once the data was captured it was cleaned so that it could be imported into SPSS for quantitative analysis and Nvivo for qualitative analysis. The cleaning included: Ensuring that each review was attached to a unique reviewer name to allow for import into Nvivo. For example, where a reviewer had written reviews about more than one park, R1, R2, R3 and so on were added to their name: “AussieGirl R1”, “AussieGirl R2” etc. Standardising place names to ensure that spellings were correct. Allocating state and/or countries to reviewers who gave only partial location information such as city of residence. Ensuring that the data set contained no blank cells. Removing reviewers under 18 years old (Age field equal to ‘13-18’). "],
["data-exploration.html", "3 Data exploration 3.1 Summary 3.2 Missing data 3.3 Count of reviews", " 3 Data exploration 3.1 Summary Between 16/May/2004 and 02/November/2019 there were 3849 reviews on TripAdvisor meeting the criteria outline in the methods. 3.2 Missing data Some of the key data fields were left blank in many reviews. For example, 1714 (45%) reviews did not provide their gender and 1987 (52%) didn’t provide their age. Figure 3.1 shows the percentage of reviews that did not provide information against each field. Figure 3.1: A large proportion of reviews did not answer some questions, particularly Gender and Age 3.3 Count of reviews There are several variables in the data with relatively few levels: Age; City; Country; Gender; id; Month; Park; Stars; State; User; and Year. Figure 3.2 shows the most frequently occuring values in each of those variables. Figure 3.2: Most frequent values in each of the variables. Most reviews are from Adelaide based 50-64 year olds called Charmaine who visit Flinders Chase in January "],
["age-and-gender.html", "4 Age and gender 4.1 Model 4.2 Model diagnostics 4.3 Model predictions", " 4 Age and gender 4.1 Model Trend through time (year) in stars given by age and gender of reviewers (and the interaction of age, gender and time) was analysed using a Bayesian binomial generalised linear mixed model. The analysis was run using the rstanarm package (Stan Development Team 2016) in R (R Core Team 2019). Month (of review), Park (visited by reviewer) and State (of origin of reviewer) were treated as random effects in the analysis. Each reviewer was then assumed to provide an independent data point for the analysis. A time field was generated as \\(time = min(Year) + (max(Year) - min(Year))/2\\). The model specification was: cbind(Y, 4 - Y) ~ Gender * Age * time + (1 | Month) + (1 | Park) + , (1 | State) where Y is Stars-1. 4.2 Model diagnostics Figure 4.1 shows that each of the chains used to estimate model parameters converged around similar values for each parameter. Figure 4.1: All chains converged around the same values for each of the model paramaters (first 10 shown here) Figure 4.2 provides further support for convergence of the chains as no values are markedly different to 1.0. Figure 4.2: Rhat values show no reason for concern Figure 4.3 shows how (a sample of) the model runs (in light blue) compare with the original data (shown in dark blue). Figure 4.3: The model runs under estimate four stars and over estimate three stars relative to the data Figure 4.4 shows how the mean and standard deviation of the model runs (in light blue) compare with that of the original data (in dark blue). Figure 4.4: Collectively the model runs do a reasonable job estimating the data 4.3 Model predictions In order to better understand how the data and resulting model are able to inform management, a number (4000) of simulations were made based on the model results (strictly, draws were made from the posterior predictive distribution). Each simulation generated a prediction based on a single draw of the model parameters from their predicted distributions. Table 4.1 shows a summary of these results. Table 4.1: Summary of model results Year Age Gender Mean Median and range in 90% credible intervals 2017 18-24 Male 3.64 4 (1 to 5) 2018 18-24 Female 4.76 5 (4 to 5) 2018 25-34 Female 4.78 5 (4 to 5) 2018 25-34 Male 4.76 5 (4 to 5) 2018 35-49 Female 4.64 5 (3 to 5) 2018 35-49 Male 4.62 5 (3 to 5) 2018 50-64 Female 4.66 5 (3 to 5) 2018 50-64 Male 4.67 5 (3 to 5) 2018 65+ Female 4.61 5 (3 to 5) 2018 65+ Male 4.49 5 (3 to 5) Figure 4.5 shows a plot of the model results. Overwhelmingly, reviewers give 5 stars, irrespective of age or gender. Figure 4.5: Model results - mean credible intervals. Most reviewers give 5 stars irrespective of age or gender. The lower 95% credible intervals go to three stars in almost every case - i.e. more than 95% of responses are three stars or more. Only for 18-24 year old males in 2015 do the 95% credible intervals go to 1 star References "],
["park.html", "5 Park 5.1 Model 5.2 Model diagnostics 5.3 Model predictions", " 5 Park 5.1 Model Trend through time (year) in stars given to a park (and the interaction of park and time) was analysed using a Bayesian binomial generalised linear mixed model. The analysis was run using the rstanarm package (Stan Development Team 2016) in R (R Core Team 2019). Age, gender and origin of reviewer were treated as random effects in the analysis. Each reviewer was then assumed to provide an independent data point for the analysis. A time field was generated as \\(time = min(Year) + (max(Year) - min(Year))/2\\). The model specification was: cbind(Y, 4 - Y) ~ time * Park + (1 | Age) + (1 | Gender) + (1 | , State) where Y is Stars-1. 5.2 Model diagnostics Figure 5.1 shows that each of the chains used to estimate model parameters converged around similar values for each parameter. Figure 5.1: All chains converged around the same values for each of the model paramaters (first 10 shown here) Figure 5.2 provides further support for convergence of the chains as no values are markedly different to 1.0. Figure 5.2: Rhat values show no reason for concern Figure 5.3 shows how (a sample of) the model runs (in light blue) compare with the original data (shown in dark blue). Figure 5.3: The model runs under estimate four stars and over estimate three stars relative to the data Figure 5.4 shows how the mean and standard deviation of the model runs (in light blue) compare with that of the original data (in dark blue). Figure 5.4: Collectively the model runs do a reasonable job estimating the data 5.3 Model predictions In order to better understand how the data and resulting model are able to inform management, a number (4000) of simulations were made based on the model results (strictly, draws were made from the posterior predictive distribution). Each simulation generated a prediction based on a single draw of the model parameters from their predicted distributions. Table 5.1 shows a summary of these results. Table 5.1: Summary of model results for the last year. Point Labatt Conservation Park, Granite Island and Belair National Park have the lowest stars (based on mean credible estimates) Park Year Mean Median and range in 90% credible intervals Flinders Ranges National Park 2018 4.81 5 (4 to 5) Flinders Chase National Park 2018 4.77 5 (3 to 5) Hallett Cove Conservation Park 2018 4.71 5 (3 to 5) Morialta Conservation Park 2018 4.63 5 (3 to 5) Innes National Park 2018 4.62 5 (3 to 5) Breakaways Conservation Park 2018 4.58 5 (3 to 5) Waterfall Gully 2018 4.50 5 (3 to 5) Other 2018 4.44 5 (3 to 5) Point Labatt Conservation Park 2018 4.36 5 (2 to 5) Granite Island 2018 4.34 5 (2 to 5) Belair National Park 2018 4.28 4 (2 to 5) Figure 5.5: Model results - mean credible intervals. Belair and Point Labatt both seem to be getting increasingly negative reviews - although the mean credible estimate for both remains above 4 stars currently References "],
["origin.html", "6 Origin 6.1 Model 6.2 Model diagnostics 6.3 Model predictions", " 6 Origin 6.1 Model Trend through time (year) in stars given by the origin of reviewers (and the interaction of origin and time) was analysed using a Bayesian binomial generalised linear mixed model. The analysis was run using the rstanarm package (Stan Development Team 2016) in R (R Core Team 2019). Age and gender of the reviewer and the park visisted were treated as random effects in the analysis. Each reviewer was then assumed to provide an independent data point for the analysis. A time field was generated as \\(time = min(Year) + (max(Year) - min(Year))/2\\). The model specification was: cbind(Y, 4 - Y) ~ time * State + (1 | Age) + (1 | Gender) + (1 | , Park) where Y is Stars-1. 6.2 Model diagnostics Figure 6.1 shows that each of the chains used to estimate model parameters converged around similar values for each parameter. Figure 6.1: All chains converged around the same values for each of the model paramaters (first 10 shown here) Figure 6.2 provides further support for convergence of the chains as no values are markedly different to 1.0. Figure 6.2: Rhat values show no reason for concern Figure 6.3 shows how (a sample of) the model runs (in light blue) compare with the original data (shown in dark blue). Figure 6.3: The model runs under estimate four stars and over estimate three stars relative to the data Figure 6.4 shows how the mean and standard deviation of the model runs (in light blue) compare with that of the original data (in dark blue). Figure 6.4: Collectively the model runs do a reasonable job estimating the data 6.3 Model predictions In order to better understand how the data and resulting model are able to inform management, a number (4000) of simulations were made based on the model results (strictly, draws were made from the posterior predictive distribution). Each simulation generated a prediction based on a single draw of the model parameters from their predicted distributions. Table 6.1 shows a summary of these results. Table 6.1: Summary of model results for the last year. California and Queensland give the highest stars (based on mean credible estimates) State Year Mean Median and range in 90% credible intervals California 2017 4.91 5 (4 to 5) Queensland 2018 4.74 5 (3 to 5) New South Wales 2018 4.68 5 (3 to 5) Other 2018 4.66 5 (3 to 5) Victoria 2018 4.61 5 (3 to 5) Australian Capital Territory 2018 4.60 5 (3 to 5) England 2018 4.58 5 (3 to 5) North Island 2018 4.56 5 (3 to 5) South Australia 2018 4.54 5 (3 to 5) Western Australia 2018 4.48 5 (3 to 5) Singapore 2018 4.31 5 (2 to 5) Figure 6.5: Model results - mean credible intervals. California and the North Island of New Zealand both appear to have increased the stars they give in reviews through time, however the total number of reviews are low in each case so this result should be treated cautiously. There is very little trend in stars given by origin of reviewers for other origins References "],
["word-analysis.html", "7 Word analysis 7.1 Overall sentiment 7.2 By park sentiment", " 7 Word analysis 7.1 Overall sentiment Using the tidytext package (Robinson and Silge 2018), the text within the character fields Title and Review were analysed. Figure 7.1 shows the ten most common positive, negative and neutral words given in titles. Figure 7.2 shows the same for reviews. The following words were reclassified as neutral from negative sentiment as they were overwhelmingly not used in a negative manner: cave; challenging; cold; crashing; desert; dirt; downhill; falls; miss; remarkable; rocky; ruins; shady; steep; unbelievable; unexpected; unusual; wedge; and wild. The following words were removed from the analysis as they refer to park names or features and therefore occur frequently: adelaide; admirals; arch; breakaways; chase; coober; flinders; fur; granite; gully; harbor; harbour; island; kangaroo; ki; lofty; morialta; mount; mt; national; park; pedy; pound; ranges; remarkable; rocks; victor; waterfall; and wilpena. Some words were also replaced with a synonym (Table 7.1). Table 7.1: Words that were changed to a synonym before further analysis word changed walks walk views view disappointing dissapoint disappointed dissapoint disappointment dissapoint colours colour color colour penguins penguin walking walk beauty beautiful enjoyed enjoy loved love sadly sad difficulty difficult crowded crowd limits limit falling fall Figure 7.1: Titles had words with positive sentiment much more frequently than words with negative sentiment Figure 7.2: Reviews used words with positive sentiment more frequently than words with negative sentiment 7.2 By park sentiment Figure 7.3: Top ten words in the title text for each sentiment class (positive, negative and neutral) for the parks with the most reviews Figure 7.4: Top ten words in the review text for each sentiment class (positive, negative and neutral) for the parks with the most reviews References "],
["discussion.html", "8 Discussion 8.1 Questions 8.2 Words used in reviews", " 8 Discussion 8.1 Questions It is now possible to answer the questions posed at the start with respect to the TripAdvisor data. 8.1.1 What is the effect of age and gender on the rating given? There is very little effect of age and gender on stars. 18-24 year old males had the lowest mean credible estimate for stars but this was still a relatively healthy 3.64 stars. 8.1.2 What is the effect of the park visited on the rating given? There is little effect of park on the rating given. In the last year there was only 0.53 stars difference between the maximum and minimum mean credible estimate for stars given to parks in the dataset. This is only 10.6% of the possible range of 1 to 5. 8.1.3 What is the effect of the origin of a reviewer on the rating given? There is very little effect of the origin of a reviewer on the rating given. In the last year, there was only 0.43 stars or 8.6% of the possible difference. 8.1.4 Summary The TripAdvisor data has little resolution in stars given for the attributes available for analysis. Most reviewers give South Australian DEW managed parks very good reviews - roughly 4.5 stars on average. 8.2 Words used in reviews As for the stars analysis, the word analysis showed overwhelmingly positive words were used to describe both overall parks (Figures 7.1 &amp; 7.2) or for specific parks (Figures 7.3 &amp; 7.4). "],
["references.html", "9 References", " 9 References "]
]
